{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQ3IUvRCafOx"
      },
      "source": [
        "# Question 4.1\n",
        "\n",
        "A rock slope is to be cut as part of a road construction project. However, there is a risk that the slope could slide along the bedding planes into the excavation pit. In order to carry out a stability analysis it is necessary to estimate the angle of friction along the bedding planes. An initial assessment on the mean value of the friction angle can be obtained from the spectrum of possible manifestations of the friction angle. It cannot be smaller than the base friction angle, i.e. the friction angle that would result on the sawn, smooth surface of a specimen. The base friction angle for the greywackes that make up the slope is about 20°. The maximum possible friction angle can be estimated by back-calculating past failure cases. So far, no friction angle exceeding 35° has been back-calculated. What is the probability that the friction angle is smaller than 25°? Since no further information is available, the PDF of the friction angle can be assumed to be uniform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "torSW-6GafOz",
        "outputId": "bd7cc54b-2540-42db-e691-f64084e8812a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability of the angle of friction being less than 25 degrees is: 0.3333333333333333\n"
          ]
        }
      ],
      "source": [
        "# In this cell, you can begin by importing the libraries you need and setting up a simple\n",
        "# calculation to handle the uniform distribution between 20 and 35 degrees.\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# HINT:\n",
        "# 1. You can model friction angles as U(20, 35).\n",
        "# 2. Calculate the probability that the friction angle is below 25 degrees.\n",
        "# 3. Confirm your result matches the analytical approach: (25 - 20) / (35 - 20).\n",
        "\n",
        "# You might start coding something like this:\n",
        "# lower_bound = 20\n",
        "# upper_bound = 35\n",
        "# desired_angle = 25\n",
        "\n",
        "# Assign variables to the max, min, and desired friction angles\n",
        "phi_max = 35\n",
        "phi_min = 20\n",
        "phi_25 = 25\n",
        "\n",
        "#Defining the uniform_less_than functions\n",
        "def uniform_less_than(lower_bound, upper_bound, value):\n",
        "   return stats.uniform.cdf(value, loc=lower_bound, scale=upper_bound - lower_bound)\n",
        "\n",
        "#Using the uniform_less_than_function\n",
        "probability = uniform_less_than(phi_min, phi_max, phi_25)\n",
        "print(f\"The probability of the angle of friction being less than {phi_25} degrees is: {probability}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eww8xJhiafOz"
      },
      "source": [
        "# Question 4.2\n",
        "\n",
        "A mobile point load tester can be used to determine a strength index \\(I_p\\) [MPa] for rock samples, which are correlated with the uniaxial compressive strength. 25 strength values \\(I_p\\) were determined for a sandstone sequence:\n",
        "\n",
        "4.4; 4.2; 4.5; 4.0; 4.3; 4.1; 3.9; 4.2; 4.5; 3.8; 4.0; 4.3; 4.2; 4.2;\n",
        "4.1; 4.5; 4.2; 4.3; 4.1; 3.7; 4.0; 4.5; 4.6; 4.3; 4.1\n",
        "(n = 25)\n",
        "\n",
        "In the neighboring mapping area, another 10 strength coefficients for sandstone were determined:\n",
        "\n",
        "3.7; 4.5; 4.2; 4.4; 4.3; 4.0; 3.5; 3.9; 4.3; 4.3\n",
        "(n = 10)\n",
        "\n",
        "Do both samples come from the same population?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bLaDNiyafOz",
        "outputId": "0dde9d02-1309-4eb4-88dd-1d58d48b2a98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t-statistic: 0.9296888097975631\n",
            "p-value: 0.35928348941549637\n",
            "The data sets are not statistically different.\n"
          ]
        }
      ],
      "source": [
        "# In this cell, import the necessary libraries for statistical testing.\n",
        "# A typical approach is to use scipy.stats to perform a t-test or similar.\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# HINT:\n",
        "# 1. Store each sample in a NumPy array.\n",
        "# 2. Decide on an appropriate test (e.g., an unpaired two-sample t-test).\n",
        "# 3. Check assumptions (normality, variance).\n",
        "# 4. Use stats.ttest_ind(...) or a non-parametric alternative if needed.\n",
        "\n",
        "# Example starting code:\n",
        "# sample1 = np.array([4.4, 4.2, 4.5, ... ])  # 25 values\n",
        "# sample2 = np.array([3.7, 4.5, 4.2, ... ])  # 10 values\n",
        "# ...\n",
        "\n",
        "#Assigning data to arrays\n",
        "site_A = np.array([4.4, 4.2, 4.5, 4.0, 4.3, 4.1, 3.9, 4.2, 4.5, 3.8, 4.0, 4.3, 4.2, 4.2, 4.1, 4.5, 4.2, 4.3, 4.1, 3.7, 4.0, 4.5, 4.6, 4.3, 4.1])  #Site A assigned to array\n",
        "site_B = np.array([3.7, 4.5, 4.2, 4.4, 4.3, 4.0, 3.5, 3.9, 4.3, 4.3])       #Site B assigned to array\n",
        "\n",
        "\n",
        "#Running t-test to tell if the materials are significantly different.\n",
        "t_stat, p_value = stats.ttest_ind(site_A, site_B)\n",
        "print(f\"t-statistic: {t_stat}\")\n",
        "print(f\"p-value: {p_value}\")\n",
        "\n",
        "#If statement indicating whether or not the data sets are statistically different.\n",
        "if p_value < 0.05:\n",
        "    print(\"The data sets are statistically different.\")\n",
        "else:\n",
        "    print(\"The data sets are not statistically different.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz6hAu69afO0"
      },
      "outputs": [],
      "source": [
        "# Question 4.3\n",
        "\n",
        "A construction pit of 10,000 m³ is to be excavated. The subground consists of Pleistocene sediments of which it is known that erratic blocks (boulders) occur, having an average diameter of 1.5 m. Experience shows that about 1% by volume of the excavated material in this region consists of boulders. Since special equipment is needed to extract them and delays in construction are to be expected, the contractor is interested in the probability of having to extract more boulders than experience suggests. He would also like to know what the probability is that more than ten boulders will be found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DtY9h35qafO0",
        "outputId": "02f2ded8-7766-4732-a6a5-df92dc78b88c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V_boulders: 100.0 m^3\n",
            "V_indboulders: 1.7671458676442586 m^3\n",
            "Estimated amount of boulders: 56.58842421045168 boulders\n",
            "Probability of exceeding expected amount of boulders: 0.4964\n",
            "Probability of exceeding 10 boulders: 1.0\n"
          ]
        }
      ],
      "source": [
        "# In this cell, you can set up a binomial or Poisson approach (depending on your modeling assumptions).\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# HINT:\n",
        "# 1. Estimate the expected number of boulders as 1% of the total volume or total 'units' of excavation.\n",
        "# 2. Use a binomial model with parameters n (number of trials) and p = 0.01, or\n",
        "#    reason about the mean number of boulders and approximate with a Poisson distribution.\n",
        "# 3. Compute probabilities for \"more boulders than expected\" and for \"more than ten boulders.\"\n",
        "\n",
        "# Example:\n",
        "# n = 10000  # or adjusted for how you're counting possible 'boulder events'\n",
        "# p = 0.01\n",
        "# ...\n",
        "\n",
        "#Define Parameters\n",
        "V_total = 10000       #m^3\n",
        "Percentage_boulders = 0.01\n",
        "V_boulders = V_total * Percentage_boulders    #m^3\n",
        "D_boulder = 1.5       #m\n",
        "V_indboulders = 4*np.pi*((D_boulder/2)**3)/3     #m^3\n",
        "print(f\"V_boulders: {V_boulders} m^3\")          #total volume of boulders\n",
        "print(f\"V_indboulders: {V_indboulders} m^3\")    #individual volume of single boulder\n",
        "num_boulders = V_boulders/V_indboulders         #estimated Number of boulders\n",
        "print(f\"Estimated amount of boulders: {num_boulders} boulders\")\n",
        "\n",
        "#Creating poisson distribution around the 1% boulder by volume stat.\n",
        "samples = stats.poisson.rvs(mu=num_boulders, size=10000)\n",
        "\n",
        "#Calc probability of exceedance of expected amount of boulders\n",
        "exceed_1 = np.sum(samples > num_boulders)/len(samples)\n",
        "print(f\"Probability of exceeding expected amount of boulders: {exceed_1}\")\n",
        "\n",
        "\n",
        "#Calc probability of exceedance of 10 boulders\n",
        "exceed_2 = np.sum(samples > 10)/len(samples)\n",
        "print(f\"Probability of exceeding 10 boulders: {exceed_2}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCFgTwSxafO0"
      },
      "source": [
        "# Question 4.4\n",
        "\n",
        "In the Devonian sedimentary bedrock of the Rhenish Massif (Germany), bedding plane distances were measured for four different stratigraphic units. The coefficients of (squared) skewness and kurtosis are given as:\n",
        "\n",
        "\\[\n",
        "(\\beta_1^2, \\beta_2) = (1.82, 4.85);\\quad (1.00, 3.35);\\quad (2.72, 5.71);\\quad (0.52, 2.95).\n",
        "\\]\n",
        "\n",
        "What statistical distribution do the bedding plane distances follow?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mJeARb2AafO0",
        "outputId": "d6f478f8-a670-48d6-981c-0d3f78629744",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skew: [1.82 1.   2.72 0.52]\n",
            "Kurtosis: [4.85 3.35 5.71 2.95]\n"
          ]
        }
      ],
      "source": [
        "# In this cell, consider how to analyze a dataset to infer its distribution based on skewness and kurtosis.\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# HINT:\n",
        "# 1. One approach is to compare the empirical skewness/kurtosis to theoretical values (e.g., normal, lognormal, gamma).\n",
        "# 2. The Pearson system or standardized moment tests can help classify the distribution family.\n",
        "# 3. If you had actual data, you could apply stats.skew(...) and stats.kurtosis(...) and compare.\n",
        "\n",
        "# Example steps you might take:\n",
        "# measured_skew_kurt = [(1.82, 4.85), (1.00, 3.35), (2.72, 5.71), (0.52, 2.95)]\n",
        "# ...\n",
        "\n",
        "#Create array of skew and kurtosis\n",
        "measured_skew_kurt = [(1.82, 4.85), (1.00, 3.35), (2.72, 5.71), (0.52, 2.95)]\n",
        "sk_array = np.array(measured_skew_kurt)\n",
        "print(f\"Skew: {sk_array[:, 0]}\")\n",
        "print(f\"Kurtosis: {sk_array[:, 1]}\")\n",
        "\n",
        "#Using hint No. 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FF1u0BdafO0"
      },
      "source": [
        "### Question 4.5\n",
        "In a valley, a bone fragment of a previously unknown Cretaceous species of ichthyosaur was found during a geological excursion. To search for more fragments, you plan to investigate two upstream branches of the river. Branch 1 (larger) has a catchment area of 18 km², while Branch 2 (smaller) has a catchment area of 10 km². Additionally, in 35% of Branch 1’s area marine Cretaceous rocks are exposed, whereas in Branch 2 that figure is 80%. Given these data, estimate the probability that the fossil came from the larger catchment area. Explain any assumptions you make about probabilities and how you handle the likelihood of the fossil being transported from each branch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Xysx4c1VafO0",
        "outputId": "a1ba331b-2e20-4ee4-8ee4-0fb906527147",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exposed area: 6.3 km^2\n",
            "Exposed area: 8.0 km^2\n"
          ]
        }
      ],
      "source": [
        "# Hints and Starting Code for Question 4.5\n",
        "\n",
        "# You might want to import basic libraries such as numpy and math for probability calculations:\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Approach Hints:\n",
        "# 1. Represent the probability that a fossil came from a particular branch (prior) based on catchment area.\n",
        "# 2. Update that probability using the conditional probability of the fossil being marine Cretaceous\n",
        "#    (which depends on the fraction of each catchment area exposing marine rocks).\n",
        "# 3. Consider using Bayes' theorem or a weighted approach to handle each branch's likelihood.\n",
        "\n",
        "# You can define variables like:\n",
        "# area_branch1 = 18\n",
        "# area_branch2 = 10\n",
        "# marine_fraction_branch1 = 0.35\n",
        "# marine_fraction_branch2 = 0.80\n",
        "# Then calculate the updated probability.\n",
        "\n",
        "#Define Variables\n",
        "Area_b1 = 18\n",
        "percent_marine_b1 = 0.35\n",
        "exposed_b1 = Area_b1 * percent_marine_b1\n",
        "print(f\"Exposed area: {exposed_b1} km^2\")\n",
        "\n",
        "Area_b2 = 10\n",
        "percent_marine_b2 = 0.80\n",
        "exposed_b2 = Area_b2 * percent_marine_b2\n",
        "print(f\"Exposed area: {exposed_b2} km^2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQdbzgkeafO1"
      },
      "source": [
        "### Question 4.6\n",
        "Extensive shear strength studies on marine sand indicate that its friction angle follows a lognormal distribution. The measured mean value and standard deviation of the log-transformed friction angle are reported as μ<sub>lnφ</sub> = 3.25 and σ<sub>lnφ</sub> = 0.65. Use these parameters to discuss the distribution of friction angles and to compute key statistics (e.g., the mean friction angle in degrees and its confidence intervals) assuming a lognormal model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0UOTNvOafO1"
      },
      "outputs": [],
      "source": [
        "# Hints and Starting Code for Question 4.6\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.stats import lognorm\n",
        "\n",
        "# Approach Hints:\n",
        "# 1. Recall that if X is lognormally distributed, then ln(X) ~ N(mean, std).\n",
        "# 2. Here, mean = 3.25, std = 0.65 refer to the normal distribution of ln(φ).\n",
        "# 3. Use scipy.stats.lognorm functions or manual transformations to find mean,\n",
        "#    confidence intervals, etc., in the original friction angle space.\n",
        "#\n",
        "# Example steps:\n",
        "# shape = sigma_lnphi\n",
        "# scale = np.exp(mu_lnphi)\n",
        "# Then lognorm.mean(shape, scale=scale) can give the mean of the distribution in original units.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2anIoidQafO1"
      },
      "source": [
        "### Question 4.7\n",
        "Ten shear tests on a Tertiary clay deposit yield friction angle (φ) and cohesion (c) pairs:\n",
        "(25°, 50 kN/m²), (22°, 65 kN/m²), (32°, 18 kN/m²), (29°, 20 kN/m²), (28°, 30 kN/m²), (38°, 5 kN/m²), (36°, 6 kN/m²), (32°, 12 kN/m²), (27°, 38 kN/m²), (23°, 45 kN/m²). Investigate whether there is a correlation between friction angle and cohesion. Summarize your findings on the relationship (e.g., positive, negative, or none).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQZuCXasafO1"
      },
      "outputs": [],
      "source": [
        "# Hints and Starting Code for Question 4.7\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Approach Hints:\n",
        "# 1. Store the friction angle and cohesion data in numpy arrays or a pandas DataFrame.\n",
        "# 2. Plot φ vs. c to visualize potential correlation.\n",
        "# 3. Calculate Pearson's correlation coefficient using pearsonr or a similar function.\n",
        "# 4. Interpret whether the correlation is statistically significant and positive/negative.\n",
        "\n",
        "# Example structure:\n",
        "# phi = np.array([25, 22, 32, 29, 28, 38, 36, 32, 27, 23])\n",
        "# cohesion = np.array([50, 65, 18, 20, 30, 5, 6, 12, 38, 45])\n",
        "# correlation_coefficient, p_value = pearsonr(phi, cohesion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTPvkJRiafO1"
      },
      "outputs": [],
      "source": [
        "### Question 4.8\n",
        "Monthly landslide frequency in a mountain region is documented in a histogram (Fig. 4.53). The question is whether there is a cyclic pattern or seasonal trend in landslide occurrence. Outline a method to determine if the observed frequencies are random or exhibit significant seasonality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZHBFkQHafO1"
      },
      "outputs": [],
      "source": [
        "# Hints and Starting Code for Question 4.8\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Approach Hints:\n",
        "# 1. Represent the monthly frequency data in a time series format.\n",
        "# 2. Use time-series analysis, e.g., seasonal_decompose from statsmodels, to detect seasonality.\n",
        "# 3. Alternatively, consider hypothesis tests for randomness or periodicity (e.g. autocorrelation plots).\n",
        "# 4. Visualize the data to check for any repeating patterns across months or seasons.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg-huBd4afO1"
      },
      "source": [
        "### Question 4.9\n",
        "Two boreholes have been drilled on opposite sides of a fault. The objective is to reconstruct the vertical displacement across the fault. Direct markers are lacking, but variations in mean chlorite content (Fig. 4.54) may provide an indirect measure of displacement. Propose a strategy to use cross-correlation of chlorite content profiles to estimate the fault offset. Discuss any assumptions about continuity and variability of the chlorite data in the subsurface.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBZZZplKafO1"
      },
      "outputs": [],
      "source": [
        "# Hints and Starting Code for Question 4.9\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import correlate\n",
        "\n",
        "# Approach Hints:\n",
        "# 1. Represent each borehole's chlorite content profile as a 1D series of numeric values at depth intervals.\n",
        "# 2. Use the scipy.signal.correlate function to compute cross-correlation between the two profiles.\n",
        "# 3. Identify the lag (depth shift) that maximizes correlation as an estimate of displacement.\n",
        "# 4. Carefully consider sampling intervals, data resolution, and boundary effects.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFelIWV6afO1"
      },
      "source": [
        "### Question 4.10\n",
        "A medieval copper mine site is being converted into residential housing, and evidence of ancient copper contamination is expected. Sea thrift (Armeria maritima) is an indicator plant for copper, so its presence was recorded along a profile in 20 m × 20 m squares. The observed shoot counts are:  \n",
        "3, 5, 11, 12, 8, 19, 22, 18, 11, 13  \n",
        "\n",
        "An experimental semivariogram is to be derived from these data as a preliminary spatial analysis. Outline how you would compute and plot a semivariogram for these shoot counts, and discuss what patterns in spatial variability might imply for copper contamination distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZNBROmJafO1"
      },
      "outputs": [],
      "source": [
        "# Hints and Starting Code for Question 4.10\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Approach Hints:\n",
        "# 1. Arrange the shoot counts in spatial order (e.g., as a function of distance along the profile).\n",
        "# 2. Calculate pairwise distances between measurement points (assuming each square is at intervals of 20 m).\n",
        "# 3. Compute the semivariogram γ(h) = 0.5 * mean[ (Z(x) - Z(x+h))^2 ] for each distance bin h.\n",
        "# 4. Plot the semivariogram (γ on the y-axis vs. distance h on the x-axis) to see if there's any spatial structure.\n",
        "# 5. Look for a sill, range, or nugget effect that might indicate how contamination (and thus sea thrift) is distributed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG52JDLMafO2"
      },
      "source": [
        "# Question 4.11\n",
        "\n",
        "The following rock sequences are encountered in exploratory drilling:\n",
        "\n",
        "- Claystone (C)  \n",
        "- Conglomerate (K)  \n",
        "- Sandstone (S)  \n",
        "- Siltstone (U)\n",
        "\n",
        "(See Table 4.5 for the per-meter breakdown.)\n",
        "\n",
        "**Task:**  \n",
        "1. What is the probability that conglomerate (K) will be drilled again in the *next* meter of drilling?  \n",
        "2. What is the probability that conglomerate will be drilled again in the *next-but-one* meter of drilling?\n",
        "\n",
        "You may assume that the encountered rock types can be treated as sequential observations of a Markov process or using simpler independence assumptions (depending on your interpretation of the problem’s statement)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8McnChhafO2"
      },
      "outputs": [],
      "source": [
        "# Use this cell to start setting up your approach in Python.\n",
        "# Possible steps:\n",
        "# 1. Represent the rock sequences as a list or array.\n",
        "# 2. Construct transition probabilities from the data if needed.\n",
        "# 3. Calculate probabilities for \"next meter\" and \"next-but-one meter.\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# HINT:\n",
        "# - If you treat each meter drilled as a step in a Markov chain, you need a 4x4 transition matrix\n",
        "#   (C, K, S, U) or some simpler approach based on frequencies.\n",
        "# - Probability that K appears next might come directly from the row in the matrix corresponding\n",
        "#   to the current rock type.\n",
        "# - For the 'next-but-one' question, you might multiply transition matrices (e.g., T^2)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oN-nHIvafO2"
      },
      "source": [
        "# Question 4.12\n",
        "\n",
        "The following orientation data (dip direction α / dip β) for bedding planes are recorded (n=14):\n",
        "\n",
        "123/25, 147/22, 120/24, 111/24, 142/26, 133/27, 135/22, 156/21,  \n",
        "110/25, 101/25, 133/20, 123/22, 145/25, 126/24\n",
        "\n",
        "**Task:**  \n",
        "1. Determine the mean resultant length.  \n",
        "2. Determine the dip direction and dip of the mean vector.  \n",
        "3. Determine the spherical standard deviation and the spherical confidence interval of the mean vector for an error probability of α = 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc-OdEVnafO2"
      },
      "outputs": [],
      "source": [
        "# Use this cell to load and process the orientation data.\n",
        "# HINT:\n",
        "# 1. Convert the dip direction/dip pairs into unit vectors in 3D space.\n",
        "# 2. Sum these vectors, and derive the resultant direction and magnitude.\n",
        "# 3. Calculate relevant statistics (standard deviation, confidence intervals) on a sphere.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Example structure:\n",
        "# data = [(123,25), (147,22), (120,24), ...]\n",
        "# Then convert each (alpha, beta) to x,y,z components on the unit sphere.\n",
        "# Summation and geometry yield the mean vector direction and length.\n",
        "#\n",
        "# You might consider using spherical to Cartesian conversions:\n",
        "# x = cos(dip) * sin(direction)\n",
        "# y = cos(dip) * cos(direction)\n",
        "# z = sin(dip)\n",
        "# (in radians)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}